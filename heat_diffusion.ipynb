{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from scorefield.models.ddpm.denoising_diffusion import Unet\n",
    "from scorefield.models.heat.heat_diffusion import HeatDiffusion\n",
    "from scorefield.utils.rl_utils import load_config\n",
    "from scorefield.utils.utils import (\n",
    "    gen_goals, overlay_goal, overlay_multiple, combine_objects, overlay_images,\n",
    "    overlay_goal_agent, overlay_goals_agent, log_num_check,\n",
    "    gen_obstacles, gen_obstacle_masks,\n",
    ")\n",
    "from scorefield.utils.diffusion_utils import bilinear_interpolate, bilinear_interpolate_batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "config_dir = \"./scorefield/configs/heat_diffusion.yaml\"\n",
    "args = load_config(config_dir)\n",
    "device = args['device']\n",
    "\n",
    "model_path = os.path.join(args['log_path'], args['model_path'])\n",
    "\n",
    "# map_img = Image.open(\"map.png\")\n",
    "bg = Image.open('assets/toy_exp/background0.png')\n",
    "wastes = []\n",
    "wastes.append(Image.open('assets/toy_exp/waste0.png'))\n",
    "wastes.append(Image.open('assets/toy_exp/waste4.png'))\n",
    "wastes.append(Image.open('assets/toy_exp/waste5.png'))\n",
    "\n",
    "epochs = args['epochs']\n",
    "batch_size = args['batch_size']\n",
    "goal_num = len(wastes)\n",
    "goal_bounds = args['goal_bounds']\n",
    "agent_bounds = args['agent_bounds']\n",
    "eval_samples = args['eval_samples']\n",
    "obstacle_pos = args['obstacles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet2D(Unet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        out_dim, \n",
    "        dim_mults=(1, 2, 4, 8)\n",
    "    ):\n",
    "        super().__init__(dim=dim, out_dim=out_dim, dim_mults=dim_mults)\n",
    "\n",
    "    def forward(self, obs, t, x_t:Optional[torch.Tensor]=None):\n",
    "        score_map = super().forward(obs, t)\n",
    "        if x_t is not None:\n",
    "            score = bilinear_interpolate_batch(score_map, x_t)    # output: (B,2)\n",
    "            return score\n",
    "        else:\n",
    "            return score_map.permute(0, 2, 3, 1)\n",
    "\n",
    "img_size = args['image_size']\n",
    "noise_steps = args['noise_steps']\n",
    "train_lr = args['train_lr']\n",
    "    \n",
    "model = Unet2D(\n",
    "    dim=img_size,\n",
    "    out_dim = 2,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    ").to(device)\n",
    "\n",
    "diffusion = HeatDiffusion(\n",
    "    image_size=img_size,\n",
    "    noise_steps=noise_steps\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [00:03<18:20:19,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: 0.8194178342819214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 485/20000 [20:25<13:35:38,  2.51s/it]"
     ]
    }
   ],
   "source": [
    "# Train with single goal\n",
    "bg = gen_obstacles(bg, obstacle_pos)\n",
    "obstacle_masks = gen_obstacle_masks(batch_size, bg.size, img_size, obstacle_pos)\n",
    "\n",
    "for iters in tqdm(range(epochs)):    \n",
    "    goal = (torch.rand(batch_size, 2, device=device, dtype=torch.float32) * 0.2 - 0.1) \\\n",
    "                * 0.1 + torch.tensor([[-0.7, -0.7]] * batch_size, device=device, dtype=torch.float32)\n",
    "    goal = goal.unsqueeze(-2)\n",
    "    obs = overlay_goal(bg, img_size, wastes, goal)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    t = diffusion.sample_timesteps(batch_size).to(device)\n",
    "    \n",
    "    score_field = diffusion.forward_diffusion(goal, t, obstacle_masks)\n",
    "    score_pred = model(obs, t)\n",
    "    \n",
    "    loss = F.mse_loss(score_field, score_pred)\n",
    "    # loss = F.l1_loss(score_field, score_pred)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if iters % 1000 == 0:\n",
    "        print(f\"iter {iters}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 80, 196, 102, 489,  23, 403, 436, 183, 331, 140, 272, 469,  61,  16,\n",
      "        218,  90, 205, 216, 148, 369, 445, 434, 189,  94], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgYAAAH5CAYAAAD6E/bxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjhElEQVR4nO3df3BV5Z348U8CEhBJUnBISCU122UGrNRfKEadXVszi+iorGxdHLql1pFtC63ITAWs4LoqQddVFqWwOi3qLNTWGaXKbHFosLLOhvBL3PoLcWSEkSZslyUBLAHJ+f6xXx+5iAh6w4Xm9Zq5Mzk/cnjyjOLb55ybW5RlWRYAABFRXOgBAADHD2EAACTCAABIhAEAkAgDACARBgBAIgwAgKR7oQfwWXR0dMTWrVujT58+UVRUVOjhAMBxLcuy2LlzZ1RVVUVx8eHXBE7IMNi6dWsMHDiw0MMAgBPKli1b4rTTTjvsOSdkGPTp0yciIm695ZYoKSkp8GgA4PjW3t4e9z34YPrv5+GckGHw4e2DkpKS6NmzZ4FHAwAnhiO5/e7hQwAgEQYAQCIMAIBEGAAAiTAAABJhAAAkwgAASIQBAJAIAwAgEQYAQCIMAIBEGAAAiTAAABJhAAAkwgAASIQBAJAIAwAgEQYAQCIMAIBEGAAAiTAAAJKjDoMVK1bEVVddFVVVVVFUVBSLFy9Ox/bt2xdTpkyJoUOHRu/evaOqqiq+9a1vxdatW3OusX379hg7dmyUlpZGeXl53HjjjbFr167P/cMAAJ/PUYfB7t2746yzzoq5c+d+7Nj7778f69ati+nTp8e6devi6aefjg0bNsTVV1+dc97YsWPjtddei2XLlsWSJUtixYoVMX78+M/+UwAAedH9aL9h5MiRMXLkyEMeKysri2XLluXse/jhh+OCCy6IzZs3R3V1dbzxxhuxdOnSWL16dQwbNiwiIh566KG44oor4v7774+qqqqPXbe9vT3a29vTdltb29EOGwA4Ap3+jEFra2sUFRVFeXl5REQ0NjZGeXl5ioKIiLq6uiguLo6mpqZDXqO+vj7KysrSa+DAgZ09bADokjo1DPbs2RNTpkyJ66+/PkpLSyMiorm5Ofr3759zXvfu3aNv377R3Nx8yOtMmzYtWltb02vLli2dOWwA6LKO+lbCkdq3b19cd911kWVZzJs373Ndq6SkJEpKSvI0MgDgk3RKGHwYBe+++24sX748rRZERFRWVsa2bdtyzv/ggw9i+/btUVlZ2RnDAQCOUN5vJXwYBRs3bozf/OY30a9fv5zjtbW1sWPHjli7dm3at3z58ujo6Ijhw4fnezgAwFE46hWDXbt2xdtvv522N23aFOvXr4++ffvGgAED4m/+5m9i3bp1sWTJkti/f396bqBv377Ro0ePGDJkSFx++eVx0003xfz582Pfvn0xceLEGDNmzCHfkQAAHDtHHQZr1qyJr33ta2l78uTJERExbty4+Id/+Id49tlnIyLi7LPPzvm+F154IS699NKIiFi4cGFMnDgxLrvssiguLo7Ro0fHnDlzPuOPAADky1GHwaWXXhpZln3i8cMd+1Dfvn1j0aJFR/tHAwCdzGclAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkBx1GKxYsSKuuuqqqKqqiqKioli8eHHO8SzLYsaMGTFgwIDo1atX1NXVxcaNG3PO2b59e4wdOzZKS0ujvLw8brzxxti1a9fn+kEAgM/vqMNg9+7dcdZZZ8XcuXMPefy+++6LOXPmxPz586OpqSl69+4dI0aMiD179qRzxo4dG6+99losW7YslixZEitWrIjx48d/9p8CAMiL7kf7DSNHjoyRI0ce8liWZTF79uy4/fbb45prromIiCeeeCIqKipi8eLFMWbMmHjjjTdi6dKlsXr16hg2bFhERDz00ENxxRVXxP333x9VVVWf48cBAD6PvD5jsGnTpmhubo66urq0r6ysLIYPHx6NjY0REdHY2Bjl5eUpCiIi6urqori4OJqamg553fb29mhra8t5AQD5l9cwaG5ujoiIioqKnP0VFRXpWHNzc/Tv3z/nePfu3aNv377pnIPV19dHWVlZeg0cODCfwwYA/r8T4l0J06ZNi9bW1vTasmVLoYcEAH+S8hoGlZWVERHR0tKSs7+lpSUdq6ysjG3btuUc/+CDD2L79u3pnIOVlJREaWlpzgsAyL+8hkFNTU1UVlZGQ0ND2tfW1hZNTU1RW1sbERG1tbWxY8eOWLt2bTpn+fLl0dHREcOHD8/ncACAo3TU70rYtWtXvP3222l706ZNsX79+ujbt29UV1fHpEmT4u67745BgwZFTU1NTJ8+PaqqqmLUqFERETFkyJC4/PLL46abbor58+fHvn37YuLEiTFmzBjvSACAAjvqMFizZk187WtfS9uTJ0+OiIhx48bFY489Frfeemvs3r07xo8fHzt27IhLLrkkli5dGj179kzfs3Dhwpg4cWJcdtllUVxcHKNHj445c+bk4ccBAD6PoizLskIP4mi1tbVFWVlZTJ86NSc4AICP27NnT9w1a1a0trZ+6nN6J8S7EgCAY0MYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABI8h4G+/fvj+nTp0dNTU306tUrvvzlL8ddd90VWZalc7IsixkzZsSAAQOiV69eUVdXFxs3bsz3UACAo5T3MLj33ntj3rx58fDDD8cbb7wR9957b9x3333x0EMPpXPuu+++mDNnTsyfPz+ampqid+/eMWLEiNizZ0++hwMAHIXu+b7gf/7nf8Y111wTV155ZUREnH766fHzn/88Vq1aFRH/t1owe/bsuP322+Oaa66JiIgnnngiKioqYvHixTFmzJh8DwkAOEJ5XzG46KKLoqGhId56662IiHjllVfipZdeipEjR0ZExKZNm6K5uTnq6urS95SVlcXw4cOjsbHxkNdsb2+Ptra2nBcAkH95XzGYOnVqtLW1xeDBg6Nbt26xf//+uOeee2Ls2LEREdHc3BwRERUVFTnfV1FRkY4drL6+Pu688858DxUAOEjeVwx++ctfxsKFC2PRokWxbt26ePzxx+P++++Pxx9//DNfc9q0adHa2ppeW7ZsyeOIAYAP5X3F4Ec/+lFMnTo1PSswdOjQePfdd6O+vj7GjRsXlZWVERHR0tISAwYMSN/X0tISZ5999iGvWVJSEiUlJfkeKgBwkLyvGLz//vtRXJx72W7dukVHR0dERNTU1ERlZWU0NDSk421tbdHU1BS1tbX5Hg4AcBTyvmJw1VVXxT333BPV1dXxla98JV5++eV44IEH4jvf+U5ERBQVFcWkSZPi7rvvjkGDBkVNTU1Mnz49qqqqYtSoUfkeDgBwFPIeBg899FBMnz49vv/978e2bduiqqoq/v7v/z5mzJiRzrn11ltj9+7dMX78+NixY0dccsklsXTp0ujZs2e+hwMAHIWi7MBfSXiCaGtri7Kyspg+daqYAIBPsWfPnrhr1qxobW2N0tLSw57rsxIAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIOiUM3nvvvfjmN78Z/fr1i169esXQoUNjzZo16XiWZTFjxowYMGBA9OrVK+rq6mLjxo2dMRQA4CjkPQz+93//Ny6++OI46aST4te//nW8/vrr8c///M/xhS98IZ1z3333xZw5c2L+/PnR1NQUvXv3jhEjRsSePXvyPRwA4Ch0z/cF77333hg4cGAsWLAg7aupqUlfZ1kWs2fPjttvvz2uueaaiIh44oknoqKiIhYvXhxjxozJ95AAgCOU9xWDZ599NoYNGxbf+MY3on///nHOOefEo48+mo5v2rQpmpubo66uLu0rKyuL4cOHR2Nj4yGv2d7eHm1tbTkvACD/8h4G77zzTsybNy8GDRoUzz//fHzve9+LH/7wh/H4449HRERzc3NERFRUVOR8X0VFRTp2sPr6+igrK0uvgQMH5nvYAEB0Qhh0dHTEueeeGzNnzoxzzjknxo8fHzfddFPMnz//M19z2rRp0draml5btmzJ44gBgA/lPQwGDBgQZ5xxRs6+IUOGxObNmyMiorKyMiIiWlpacs5paWlJxw5WUlISpaWlOS8AIP/yHgYXX3xxbNiwIWffW2+9FV/60pci4v8eRKysrIyGhoZ0vK2tLZqamqK2tjbfwwEAjkLe35Vwyy23xEUXXRQzZ86M6667LlatWhWPPPJIPPLIIxERUVRUFJMmTYq77747Bg0aFDU1NTF9+vSoqqqKUaNG5Xs4AMBRyHsYnH/++fHMM8/EtGnT4h//8R+jpqYmZs+eHWPHjk3n3HrrrbF79+4YP3587NixIy655JJYunRp9OzZM9/DAQCOQlGWZVmhB3G02traoqysLKZPnSomAOBT7NmzJ+6aNStaW1s/9Tk9n5UAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkAgDACARBgBAIgwAgEQYAACJMAAAEmEAACTCAABIhAEAkHQv9ACOta8ftH3h1bnbM589ZkMBgOOOFQMAIOn0MJg1a1YUFRXFpEmT0r49e/bEhAkTol+/fnHKKafE6NGjo6WlpbOHAgB8ik4Ng9WrV8e//uu/xle/+tWc/bfccks899xz8dRTT8WLL74YW7dujWuvvbYzhwIAHIFOe8Zg165dMXbs2Hj00Ufj7rvvTvtbW1vjpz/9aSxatCi+/vX/u+O/YMGCGDJkSKxcuTIuvPDCvI+l5oCvL+yde8wzBQDwkU5bMZgwYUJceeWVUVdXl7N/7dq1sW/fvpz9gwcPjurq6mhsbDzktdrb26OtrS3nBQDkX6esGDz55JOxbt26WL169ceONTc3R48ePaK8vDxnf0VFRTQ3Nx/yevX19XHnnXd2xlABgAPkfcVgy5YtcfPNN8fChQujZ8+eebnmtGnTorW1Nb22bNmSl+sCALnyvmKwdu3a2LZtW5x77rlp3/79+2PFihXx8MMPx/PPPx979+6NHTt25KwatLS0RGVl5SGvWVJSEiUlJZ95TAc+YxA3HXRw9me+LAD8ycl7GFx22WXxu9/9LmffDTfcEIMHD44pU6bEwIED46STToqGhoYYPXp0RERs2LAhNm/eHLW1tfkeDgBwFPIeBn369IkzzzwzZ1/v3r2jX79+af+NN94YkydPjr59+0ZpaWn84Ac/iNra2k55RwIAcOQK8iuRH3zwwSguLo7Ro0dHe3t7jBgxIn7yk5902p+Xcyuh7IsHHX2v0/5cADjRHJMw+O1vf5uz3bNnz5g7d27MnTv3WPzxAMAR8lkJAEAiDACApMt97PLMOz1TAACfxIoBAJAIAwAg6XK3Em67446c7Zk+gwEAEisGAEAiDACARBgAAMmf5DMGNQdtV/T+6GvPFADAJ7NiAAAkwgAASIQBAJAIAwAgEQYAQCIMAIBEGAAAiTAAABJhAAAkwgAASIQBAJAIAwAgEQYAQCIMAIBEGAAAiTAAABJhAAAk3Qs9gM6w6aDtlt0ffX3bHXfkHJt5552dPyAAOEFYMQAAEmEAACTCAABI/iSfMTgczxQAwCezYgAAJMIAAEi63K2E2+64Kmd75p3PFWgkAHD8sWIAACTCAABIhAEAkHSJZwwO/BXJFbGuYOMAgOOdFQMAIBEGAEAiDACApEs8Y5Dj5fcKPQIAOG5ZMQAAEmEAACRd4lbCgW9XvLBgowCA458VAwAgEQYAQCIMAICkSzxjkKOh0AMAgOOXFQMAIBEGAEDSJW4lbDrMsdvuuCNne+add3buYADgOGbFAABIhAEAkAgDACDpEs8YHGjl7txtvyIZAD5ixQAASIQBAJAIAwAg6XLPGHzMy35vAQB8yIoBAJDkPQzq6+vj/PPPjz59+kT//v1j1KhRsWHDhpxz9uzZExMmTIh+/frFKaecEqNHj46WlpZ8DwUAOEp5D4MXX3wxJkyYECtXroxly5bFvn374q/+6q9i9+6P3id4yy23xHPPPRdPPfVUvPjii7F169a49tpr8z2UQ9p00AsA+EjenzFYunRpzvZjjz0W/fv3j7Vr18Zf/MVfRGtra/z0pz+NRYsWxde//vWIiFiwYEEMGTIkVq5cGRde+PHfLNDe3h7t7e1pu62tLd/DBgDiGDxj0NraGhERffv2jYiItWvXxr59+6Kuri6dM3jw4Kiuro7GxsZDXqO+vj7KysrSa+DAgZ09bADokjo1DDo6OmLSpElx8cUXx5lnnhkREc3NzdGjR48oLy/PObeioiKam5sPeZ1p06ZFa2trem3ZsqUzhw0AXVanvl1xwoQJ8eqrr8ZLL730ua5TUlISJSUleRrVQRo657IAcCLqtBWDiRMnxpIlS+KFF16I0047Le2vrKyMvXv3xo4dO3LOb2lpicrKys4aDgBwBPIeBlmWxcSJE+OZZ56J5cuXR01NTc7x8847L0466aRoaPjof9U3bNgQmzdvjtra2nwPBwA4Cnm/lTBhwoRYtGhR/OpXv4o+ffqk5wbKysqiV69eUVZWFjfeeGNMnjw5+vbtG6WlpfGDH/wgamtrD/mOhHw7+C2KLQd92uJtd3wxfT3zzvc6fTwAcDzJexjMmzcvIiIuvfTSnP0LFiyIb3/72xER8eCDD0ZxcXGMHj062tvbY8SIEfGTn/wk30MBAI5S3sMgy7JPPadnz54xd+7cmDt3br7/eADgc/BZCQBA0uU/XfHgZw4qWj1XAEDXZcUAAEiEAQCQCAMAIPGMwUHbFz5akGEAwHHBigEAkAgDACDp8rcSDnbgr0i+7Y6rco7NvPO5YzwaADi2rBgAAIkwAAASYQAAJF3+GYOD36544HZFq2cKAOharBgAAIkwAACSLn8r4bD8FkQAuhgrBgBAIgwAgEQYAACJZwwOcuDbFWt25x677Y4vpq9n3vnesRkQABxDVgwAgEQYAACJMAAAEs8YHMbBvy654mXPFQDwp82KAQCQCAMAIHEr4SAH3z440IUNx2wYAFAQVgwAgEQYAACJMAAAEs8YHMbBzxusPOBXJN92de6xmc92+nAAoNNZMQAAEmEAACTCAABIPGNwFA585uDCw/3CAwA4QVkxAAASYQAAJG4lfEYrf5e7fdsdV+Vsz7zzuWM4GgDIDysGAEAiDACARBgAAIlnDI7CYT+S+X7PFABw4rNiAAAkwgAASNxK+IwOvq3Qsjt3+7ZJH309c3YnDwYA8sSKAQCQCAMAIBEGAEDiGYM8WX7Q9vXvFGQYAPC5WDEAABJhAAAkwgAASDxj0ElWPvvR1z6SGYAThRUDACARBgBA4lZCnvjkRQD+FFgxAAASYQAAJMIAAEg8Y9BJDnzmYOXBH8ncO3d75kHHAaBQrBgAAElBw2Du3Llx+umnR8+ePWP48OGxatWqQg4HALq8gt1K+MUvfhGTJ0+O+fPnx/Dhw2P27NkxYsSI2LBhQ/Tv379Qw+oUB3/y4nK3DgA4ThVsxeCBBx6Im266KW644YY444wzYv78+XHyySfHz372s4+d297eHm1tbTkvACD/ChIGe/fujbVr10ZdXd1HAykujrq6umhsbPzY+fX19VFWVpZeAwcOPJbDBYAuoyC3Ev7whz/E/v37o6KiImd/RUVFvPnmmx87f9q0aTF58uS03draGtXV1dHe3t7pYwWAE92H/73MsuxTzz0h3q5YUlISJSUlafvDWwn3PfhgoYYEACecnTt3RllZ2WHPKUgYnHrqqdGtW7doaWnJ2d/S0hKVlZWf+v1VVVWxZcuWyLIsqqurY8uWLVFaWtpZwz1htbW1xcCBA83PJzA/h2d+Pp05Ojzzc3jHcn6yLIudO3dGVVXVp55bkDDo0aNHnHfeedHQ0BCjRo2KiIiOjo5oaGiIiRMnfur3FxcXx2mnnZZWDkpLS/1Ddxjm5/DMz+GZn09njg7P/BzesZqfT1sp+FDBbiVMnjw5xo0bF8OGDYsLLrggZs+eHbt3744bbrihUEMCgC6vYGHwt3/7t/Hf//3fMWPGjGhubo6zzz47li5d+rEHEgGAY6egDx9OnDjxiG4dfJKSkpK44447ch5M5CPm5/DMz+GZn09njg7P/Bze8To/RdmRvHcBAOgSfIgSAJAIAwAgEQYAQCIMAIBEGAAAyQkbBnPnzo3TTz89evbsGcOHD49Vq1YVekgFUV9fH+eff3706dMn+vfvH6NGjYoNGzbknLNnz56YMGFC9OvXL0455ZQYPXr0x34ddVcxa9asKCoqikmTJqV95ifivffei29+85vRr1+/6NWrVwwdOjTWrFmTjmdZFjNmzIgBAwZEr169oq6uLjZu3FjAER87+/fvj+nTp0dNTU306tUrvvzlL8ddd92V82E0XWl+VqxYEVdddVVUVVVFUVFRLF68OOf4kczF9u3bY+zYsVFaWhrl5eVx4403xq5du47hT9F5Djc/+/btiylTpsTQoUOjd+/eUVVVFd/61rdi69atOdco+PxkJ6Ann3wy69GjR/azn/0se+2117KbbropKy8vz1paWgo9tGNuxIgR2YIFC7JXX301W79+fXbFFVdk1dXV2a5du9I53/3ud7OBAwdmDQ0N2Zo1a7ILL7wwu+iiiwo46sJYtWpVdvrpp2df/epXs5tvvjnt7+rzs3379uxLX/pS9u1vfztramrK3nnnnez555/P3n777XTOrFmzsrKysmzx4sXZK6+8kl199dVZTU1N9sc//rGAIz827rnnnqxfv37ZkiVLsk2bNmVPPfVUdsopp2T/8i//ks7pSvPz7//+79mPf/zj7Omnn84iInvmmWdyjh/JXFx++eXZWWedla1cuTL7j//4j+zP//zPs+uvv/4Y/ySd43Dzs2PHjqyuri77xS9+kb355ptZY2NjdsEFF2TnnXdezjUKPT8nZBhccMEF2YQJE9L2/v37s6qqqqy+vr6Aozo+bNu2LYuI7MUXX8yy7P/+QTzppJOyp556Kp3zxhtvZBGRNTY2FmqYx9zOnTuzQYMGZcuWLcv+8i//MoWB+cmyKVOmZJdccsknHu/o6MgqKyuzf/qnf0r7duzYkZWUlGQ///nPj8UQC+rKK6/MvvOd7+Tsu/baa7OxY8dmWda15+fg//AdyVy8/vrrWURkq1evTuf8+te/zoqKirL33nvvmI39WDhUOB1s1apVWURk7777bpZlx8f8nHC3Evbu3Rtr166Nurq6tK+4uDjq6uqisbGxgCM7PrS2tkZERN++fSMiYu3atbFv376c+Ro8eHBUV1d3qfmaMGFCXHnllTnzEGF+IiKeffbZGDZsWHzjG9+I/v37xznnnBOPPvpoOr5p06Zobm7OmaOysrIYPnx4l5ijiy66KBoaGuKtt96KiIhXXnklXnrppRg5cmREmJ8DHclcNDY2Rnl5eQwbNiydU1dXF8XFxdHU1HTMx1xora2tUVRUFOXl5RFxfMxPQX8l8mfxhz/8Ifbv3/+xz1SoqKiIN998s0CjOj50dHTEpEmT4uKLL44zzzwzIiKam5ujR48e6R+6D1VUVERzc3MBRnnsPfnkk7Fu3bpYvXr1x46Zn4h33nkn5s2bF5MnT47bbrstVq9eHT/84Q+jR48eMW7cuDQPh/p3rivM0dSpU6OtrS0GDx4c3bp1i/3798c999wTY8eOjYjo8vNzoCOZi+bm5ujfv3/O8e7du0ffvn273Hzt2bMnpkyZEtdff336dMXjYX5OuDDgk02YMCFeffXVeOmllwo9lOPGli1b4uabb45ly5ZFz549Cz2c41JHR0cMGzYsZs6cGRER55xzTrz66qsxf/78GDduXIFHV3i//OUvY+HChbFo0aL4yle+EuvXr49JkyZFVVWV+eEz27dvX1x33XWRZVnMmzev0MPJccLdSjj11FOjW7duH3tqvKWlJSorKws0qsKbOHFiLFmyJF544YU47bTT0v7KysrYu3dv7NixI+f8rjJfa9eujW3btsW5554b3bt3j+7du8eLL74Yc+bMie7du0dFRUWXnp+IiAEDBsQZZ5yRs2/IkCGxefPmiIg0D13137kf/ehHMXXq1BgzZkwMHTo0/u7v/i5uueWWqK+vjwjzc6AjmYvKysrYtm1bzvEPPvggtm/f3mXm68MoePfdd2PZsmVptSDi+JifEy4MevToEeedd140NDSkfR0dHdHQ0BC1tbUFHFlhZFkWEydOjGeeeSaWL18eNTU1OcfPO++8OOmkk3Lma8OGDbF58+YuMV+XXXZZ/O53v4v169en17Bhw2Ls2LHp6648PxERF1988cfe4vrWW2/Fl770pYiIqKmpicrKypw5amtri6ampi4xR++//34UF+f+VdmtW7fo6OiICPNzoCOZi9ra2tixY0esXbs2nbN8+fLo6OiI4cOHH/MxH2sfRsHGjRvjN7/5TfTr1y/n+HExP8fkEcc8e/LJJ7OSkpLssccey15//fVs/PjxWXl5edbc3FzooR1z3/ve97KysrLst7/9bfb73/8+vd5///10zne/+92suro6W758ebZmzZqstrY2q62tLeCoC+vAdyVkmflZtWpV1r179+yee+7JNm7cmC1cuDA7+eSTs3/7t39L58yaNSsrLy/PfvWrX2X/9V//lV1zzTV/sm/HO9i4ceOyL37xi+ntik8//XR26qmnZrfeems6pyvNz86dO7OXX345e/nll7OIyB544IHs5ZdfTk/VH8lcXH755dk555yTNTU1ZS+99FI2aNCgP5m3Kx5ufvbu3ZtdffXV2WmnnZatX78+5+/s9vb2dI1Cz88JGQZZlmUPPfRQVl1dnfXo0SO74IILspUrVxZ6SAUREYd8LViwIJ3zxz/+Mfv+97+ffeELX8hOPvnk7K//+q+z3//+94UbdIEdHAbmJ8uee+657Mwzz8xKSkqywYMHZ4888kjO8Y6Ojmz69OlZRUVFVlJSkl122WXZhg0bCjTaY6utrS27+eabs+rq6qxnz57Zn/3Zn2U//vGPc/4i70rz88ILLxzy75xx48ZlWXZkc/E///M/2fXXX5+dcsopWWlpaXbDDTdkO3fuLMBPk3+Hm59NmzZ94t/ZL7zwQrpGoeenKMsO+PVdAECXdsI9YwAAdB5hAAAkwgAASIQBAJAIAwAgEQYAQCIMAIBEGAAAiTAAABJhAAAkwgAASP4fdXVvMZqKA4MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(t)\n",
    "i = -4\n",
    "ma = score_field[i] > 0\n",
    "mi = score_field[i][ma].min()\n",
    "diffusion.visualize_distribution(score_field[i].unsqueeze(0).cpu().numpy(),threshold=float(mi)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train with single + multiple goals\n",
    "\n",
    "train_goal_num = goal_num * (2**(goal_num - 1)) # if 3 goals: 12\n",
    "train_goals = [len(list(itertools.combinations([i for i in range(goal_num)], i+1))* (i+1)) \\\n",
    "               for i in range(goal_num)] # if 3 goals: [3, 6, 3]\n",
    "train_comb = [len(list(itertools.combinations([i for i in range(goal_num)], i+1))) \\\n",
    "              for i in range(goal_num)] # just a list of combinations of goal num. if 3: [3, 3, 1]\n",
    "\n",
    "assert batch_size % train_goal_num == 0, 'batch size has to be divided by the goal number'\n",
    "n = batch_size // train_goal_num  # if 3 goals: batch -> n * 12 \n",
    "\n",
    "for iters in tqdm(range(train_goal_num * epochs)):\n",
    "    gs = random.randrange(goal_num)        \n",
    "    goals = gen_goals(goal_bounds, train_goal_num)\n",
    "    \n",
    "    expanded_goals = goals.unsqueeze(1).expand(-1, n, -1)\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    random_offsets = (torch.rand(*expanded_goals.shape, device=goals.device, dtype=goals.dtype) * 2 - 1.) * 0.1\n",
    "    goal_pos = (expanded_goals + random_offsets).view(-1,2)\n",
    "    obs = []\n",
    "    st = 0\n",
    "    for i, t in enumerate(train_goals):\n",
    "        if i == 0: \n",
    "            obs.append(overlay_image(bg, img_size, wastes, goal_pos[st:st + n * t]))\n",
    "        else:\n",
    "            obs.append(overlay_images(bg, img_size, wastes, goal_pos[st:st + n * t], i+1))\n",
    "        st = st + n * t\n",
    "    x0 = goal_pos.unsqueeze(1)\n",
    "    obs = torch.cat(obs, dim=0)\n",
    "    \n",
    "    # duplicate obs to match the size with x0\n",
    "    duplicates = []\n",
    "    for i, tr in enumerate(train_comb):\n",
    "        if i > 0: \n",
    "            duplicates.append(obs[n*prev_tr:n*(prev_tr + tr)].repeat(i,1,1,1))\n",
    "            tr = prev_tr + tr\n",
    "        prev_tr = tr\n",
    "\n",
    "    # TODO: Below new_obs is only capable when the goal number is 3.\n",
    "    comb_sum = [0] + [n*(train_comb[c]+train_comb[c+1]) for c in range(len(train_comb)-1)]\n",
    "    new_obs = torch.cat([obs[comb_sum[n//2]:comb_sum[n//2+1]] if n%2 == 0 else duplicates[n-1] \\\n",
    "                         for n in range(len(comb_sum)-1)*2], dim=0)\n",
    "#     new_obs = torch.cat([\n",
    "#         obs[:n*(train_comb[0]+train_comb[1])],\n",
    "#         duplicates[0],\n",
    "#         obs[n*(train_comb[0]+train_comb[1]):n*(train_comb[0]+train_comb[1]+train_comb[2])],\n",
    "#         duplicates[1],\n",
    "#         obs[14:]\n",
    "#     ], dim=0)\n",
    "        \n",
    "    t = diffusion.sample_timesteps(batch_size).to(device)\n",
    "    \n",
    "    x_noisy, noise = diffusion.forward_diffusion(x0, t)\n",
    "    noise_pred = model(new_obs, x_noisy, t)\n",
    "    loss =  F.l1_loss(noise, noise_pred)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if iters % (epochs//10) == 0:\n",
    "        print(f\"iter {iters}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scorefield.utils.utils import log_num_check, get_url_pretrained\n",
    "\n",
    "model_pth = f\"./logs/pretrained/heat_{noise_steps}.pt\"\n",
    "model_pth = log_num_check(model_pth)\n",
    "torch.save(model.state_dict(), model_pth)\n",
    "\n",
    "# url = 'https://drive.google.com/uc?export=download&id=1CtqczM5cry7wg4poiCv_UeKfIrLtrx9V'\n",
    "# get_url_pretrained(url, 'model.pt')\n",
    "# model.load_state_dict(torch.load(f'./logs/pretrained/heat_{noise_steps}_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_samples=1000\n",
    "bg = gen_obstacles(bg, obstacle_pos)\n",
    "new_goals = torch.tensor([[[-0.7, -0.7]]]*3, device=device, dtype=torch.float32)\n",
    "obs_T = overlay_goal(bg, img_size, wastes, new_goals) # (goal_num, 3, H, W)\n",
    "\n",
    "c = 10\n",
    "fig,axs = plt.subplots(3, c + 1, figsize=(20,20))\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 4\n",
    "\n",
    "model.eval()\n",
    "ims = []\n",
    "with torch.no_grad():\n",
    "    x_T = torch.tensor([[[0.7, -0.7]]]*3, device=device, dtype=torch.float32)\n",
    "    # x_T = gen_goals(agent_bounds, n=(goal_num, 1))\n",
    "    x = x_T\n",
    "    \n",
    "    for i in tqdm(reversed(range(1, noise_steps)), position=0):\n",
    "        bkg = bg.copy()\n",
    "        im = overlay_goal_agent(bkg, wastes, new_goals.cpu(), x.cpu(), dot_size)\n",
    "        ims.append(im)\n",
    "\n",
    "        if (T-1-i) % step_size == 0:\n",
    "            k = (T-1-i) // step_size\n",
    "            backg = bg.copy()\n",
    "            img_sample = overlay_goal_agent(backg, wastes, new_goals.cpu(), x.cpu(), dot_size)\n",
    "            for s in range(len(img_sample)):\n",
    "                axs[s,k].imshow(img_sample[s])\n",
    "                axs[0,k].set_title(f't = {T-1-i}')\n",
    "                axs[s,k].axis('off')\n",
    "            print((score) * torch.sqrt(t) / 10)\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "        while True:\n",
    "            score = model(obs_T, t, x)\n",
    "            \n",
    "            x = x + ((score) * torch.sqrt(t) / 10) + (torch.randn_like(x) * torch.sqrt(torch.sqrt(t)) * 0.1)\n",
    "            \n",
    "            mask = (x.abs() > 1)\n",
    "            x[mask] = torch.clamp(x[mask], min=-.9, max=.9)\n",
    "            if (x.abs() <=1).all():\n",
    "                break\n",
    "\n",
    "    backg = bg.copy()\n",
    "    img_sample = overlay_goal_agent(backg, wastes, new_goals.cpu(),x.cpu(), dot_size)\n",
    "    for s in range(len(img_sample)):\n",
    "        axs[s,-1].imshow(img_sample[s])\n",
    "        axs[0,-1].set_title(f't = {T}')\n",
    "        axs[s,-1].axis('off')\n",
    "\n",
    "    bkg = bg.copy()\n",
    "    im = overlay_goal_agent(bkg, wastes, new_goals.cpu(), x.cpu(), dot_size)       \n",
    "    ims.append(im)\n",
    "#     x_trace.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/heat/single_goal/img_list.npy', ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 10\n",
    "fig,axs = plt.subplots(1, c + 1, figsize=(20,20))\n",
    "axs = axs.flatten()\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 2\n",
    "\n",
    "goals = gen_goals(goal_bounds, goal_num)\n",
    "\n",
    "objs = wastes.copy()\n",
    "gs = goals.clone()\n",
    "\n",
    "obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # x_T = torch.tensor([[0.5, 0.5]], device=device, dtype=torch.float32)\n",
    "    x_T = gen_goals(agent_bounds, n=(1, eval_samples)).unsqueeze(0)\n",
    "    x = x_T\n",
    "    \n",
    "    imgs=[]\n",
    "    for i in tqdm(reversed(range(1, noise_steps)), position=0):\n",
    "        if (T-1-i) % step_size == 0:\n",
    "            k = (T-1-i) // step_size\n",
    "            img_sample = overlay_goals_agent(bg, objs, gs.cpu(), x.cpu(), dot_size)\n",
    "            axs[k].imshow(img_sample)\n",
    "            axs[k].set_title(f't = {T-1-i}')\n",
    "            axs[k].axis('off')\n",
    "            imgs.append(img_sample)\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "        while True:\n",
    "            predicted_noise = model(obs_T, x, t)\n",
    "            alpha = diffusion.alpha[t]\n",
    "            alpha_hat = diffusion.alpha_hat[t]\n",
    "            beta = diffusion.beta[t]\n",
    "            if i > 1:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) \\\n",
    "                        * predicted_noise) + torch.sqrt(beta) * noise\n",
    "            # if (abs(x[0][0]) <= 1.) & (abs(x[0][1]) <= 1.):\n",
    "            #     break\n",
    "            mask = (x.abs() > 1)\n",
    "            x[mask] = torch.clamp(x[mask], min=-.9, max=.9)\n",
    "            if (x.abs() <=1).all():\n",
    "                break\n",
    "\n",
    "#         exclude_idx = -1\n",
    "#         for i in range(len(gs)):\n",
    "#             if get_distance(x[0], gs[i]) < 0.1 and len(gs) > 1:\n",
    "#                 exclude_idx = i\n",
    "#                 break\n",
    "#         if exclude_idx > -1:\n",
    "#             objs = objs[:i] + objs[i+1:]\n",
    "#             gs = torch.cat([gs[:i], gs[i+1:]], dim=0)\n",
    "#             obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "    img_sample = overlay_goals_agent(bg, objs, gs.cpu(),x.cpu(), dot_size)\n",
    "    axs[-1].imshow(img_sample)\n",
    "    axs[-1].set_title(f't = {T}')\n",
    "    axs[-1].axis('off')\n",
    "    imgs.append(img_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/new_goals_video/img_list.npy', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
