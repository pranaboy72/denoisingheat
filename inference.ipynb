{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcs_edf/mambaforge/envs/diffusion/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt:tensor([   50,   128,   288,   578,  1058,  1922,  3200,  5408,  8978, 14450],\n",
      "       device='cuda:0')\n",
      "heat kernel std:tensor([ 5.,  8., 12., 17., 23., 31., 32., 32., 32., 32.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from scorefield.models.ddpm.denoising_diffusion import Unet\n",
    "from scorefield.models.heat.heat_diffusion import HeatDiffusion, HeatDiffusion_Revised\n",
    "from scorefield.utils.rl_utils import load_config\n",
    "from scorefield.utils.utils import (\n",
    "    gen_goals, overlay_goal, overlay_multiple, combine_objects, overlay_images,\n",
    "    overlay_goal_agent, overlay_goals_agent, log_num_check,\n",
    "    draw_obstacles_pil, convert_to_obstacle_masks,\n",
    "    randgen_obstacle_masks, draw_obstacles_pixel,\n",
    "    vector_field, clip_vectors\n",
    ")\n",
    "from scorefield.utils.diffusion_utils import bilinear_interpolate, bilinear_interpolate_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from typing import Optional\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# Args\n",
    "config_dir = \"./scorefield/configs/heat_diffusion.yaml\"\n",
    "args = load_config(config_dir)\n",
    "device = args['device']\n",
    "\n",
    "bg = Image.open('assets/toy_exp/background0.png')\n",
    "wastes = []\n",
    "wastes.append(Image.open('assets/toy_exp/waste0.png'))\n",
    "# wastes.append(Image.open('assets/toy_exp/waste4.png'))\n",
    "# wastes.append(Image.open('assets/toy_exp/waste5.png'))\n",
    "\n",
    "\n",
    "img_size = args['image_size']\n",
    "goal_bounds = args['goal_bounds']\n",
    "goal_num = len(wastes)\n",
    "agent_bounds = args['agent_bounds']\n",
    "obstacle_pos = args['obstacles']\n",
    "\n",
    "model_path = os.path.join(args['log_path'], args['model_path'])\n",
    "\n",
    "u0 = args['u0']\n",
    "min_heat_step = args['min_heat_step']\n",
    "max_heat_step = args['max_heat_step']\n",
    "noise_steps = args['noise_steps']\n",
    "sample_num = args['sample_num']\n",
    "time_type = args['time_type']\n",
    "\n",
    "train_lr = args['train_lr']\n",
    "batch_size = noise_steps #args['batch_size']\n",
    "\n",
    "class Unet2D(Unet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        out_dim, \n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "    ):\n",
    "        super().__init__(dim=dim, out_dim=out_dim, dim_mults=dim_mults)\n",
    "\n",
    "    def forward(self, obs, t, x_t:Optional[torch.Tensor]=None):\n",
    "        score_map = super().forward(obs, t)\n",
    "        if x_t is not None:\n",
    "            score = bilinear_interpolate_samples(score_map, x_t)    # output: (B,2)\n",
    "            return score, score_map.permute(0,2,3,1)\n",
    "        else:\n",
    "            return score_map.permute(0, 2, 3, 1)\n",
    "    \n",
    "model = Unet2D(\n",
    "    dim=img_size,\n",
    "    out_dim = 2,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    ").to(device)\n",
    "\n",
    "diffusion = HeatDiffusion_Revised(\n",
    "    image_size=img_size,\n",
    "    u0 = u0,\n",
    "    noise_steps=noise_steps,\n",
    "    min_heat_step=min_heat_step,\n",
    "    max_heat_step=max_heat_step,\n",
    "    time_type=time_type,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./runs/Sep11_20-23-45_mlcs-graphics/model_params.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation pre-setting\n",
    "\n",
    "# background = draw_obstacles_pil(bg, obstacle_pos)\n",
    "# obstacle_masks = convert_to_obstacle_masks(noise_steps, background[0].size, img_size, obstacle_pos)\n",
    "obstacle_masks = randgen_obstacle_masks(1, img_size)\n",
    "background = draw_obstacles_pixel(bg, obstacle_masks)\n",
    "\n",
    "obstacle_masks = obstacle_masks[0].unsqueeze(0)\n",
    "\n",
    "plt.imshow(obstacle_masks[0].cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (sum of probabilities <= 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m obs_T \u001b[39m=\u001b[39m overlay_goal(background, img_size, wastes, goals)\n\u001b[1;32m     10\u001b[0m heat_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, noise_steps\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)], device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m heat, _, _, _ \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39;49mforward_diffusion(heat_t, goals\u001b[39m.\u001b[39;49mrepeat(diffusion\u001b[39m.\u001b[39;49mnoise_steps, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), \u001b[39m1\u001b[39;49m, obstacle_masks\u001b[39m.\u001b[39;49mrepeat(diffusion\u001b[39m.\u001b[39;49mnoise_steps, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     12\u001b[0m colormap \u001b[39m=\u001b[39m cm\u001b[39m.\u001b[39mget_cmap(\u001b[39m'\u001b[39m\u001b[39mhot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_heat\u001b[39m(array):\n",
      "File \u001b[0;32m~/Desktop/diffusion/scorefield/scorefield/models/heat/heat_diffusion.py:400\u001b[0m, in \u001b[0;36mHeatDiffusion_Revised.forward_diffusion\u001b[0;34m(self, diffusion_steps, heat_sources, sample_num, obstacle_masks)\u001b[0m\n\u001b[1;32m    398\u001b[0m         heat_dts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheat_steps[diffusion_steps\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    399\u001b[0m         ut_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_ut(heat_dts, diffusion_steps, heat_sources)\n\u001b[0;32m--> 400\u001b[0m         x_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_from_heat(ut_batch, sample_num)\n\u001b[1;32m    401\u001b[0m \u001b[39m#         return ut_batch, self.convert_space(x_t, 'norm')\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[39mreturn\u001b[39;00m ut_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(ut_batch, x_t), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(ut_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_space(x_t, \u001b[39m'\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/diffusion/scorefield/scorefield/models/heat/heat_diffusion.py:362\u001b[0m, in \u001b[0;36mHeatDiffusion_Revised.sample_from_heat\u001b[0;34m(self, u, n)\u001b[0m\n\u001b[1;32m    360\u001b[0m ut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexclude_insulators(ut)    \u001b[39m# exclude samples in insulators by numerical error\u001b[39;00m\n\u001b[1;32m    361\u001b[0m ut_flat \u001b[39m=\u001b[39m ut\u001b[39m.\u001b[39mview(ut\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m),\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 362\u001b[0m sampled_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(ut_flat, n, replacement\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    364\u001b[0m h \u001b[39m=\u001b[39m sampled_indices \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m u\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)\n\u001b[1;32m    365\u001b[0m w \u001b[39m=\u001b[39m sampled_indices \u001b[39m%\u001b[39m u\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (sum of probabilities <= 0)"
     ]
    }
   ],
   "source": [
    "# Evaluation - Single goal\n",
    "\n",
    "\n",
    "lan_t = 200  # 4\n",
    "epsilon = .01\n",
    "\n",
    "goals = torch.tensor([[[0.8, 0.8]]], device=device, dtype=torch.float32)\n",
    "obs_T = overlay_goal(background, img_size, wastes, goals)\n",
    "\n",
    "heat_t = torch.tensor([i for i in range(1, noise_steps+1)], device=device)\n",
    "heat, _, _, _ = diffusion.forward_diffusion(heat_t, goals.repeat(diffusion.noise_steps, 1, 1), 1, obstacle_masks.repeat(diffusion.noise_steps, 1, 1))\n",
    "colormap = cm.get_cmap('hot')\n",
    "def normalize_heat(array):\n",
    "    min_val = array.min()\n",
    "    max_val = array.max()\n",
    "    return (array - min_val) / (max_val - min_val)\n",
    "heat_bgs = [Image.fromarray((colormap(normalize_heat(heat[h].cpu().numpy()))[:, :, :3] * 255).astype(np.uint8)) for h in range(heat.size(0))]\n",
    "\n",
    "\n",
    "c = 10\n",
    "fig, axs = plt.subplots(2, c, figsize=(20,20))\n",
    "\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 3\n",
    "\n",
    "\n",
    "model.eval()\n",
    "ims = []\n",
    "ims_heat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.tensor([[[0., 0.]]], device=device, dtype=torch.float32)\n",
    "    x = x_T\n",
    "    \n",
    "    for i in tqdm(reversed(range(1, T+1)), position=0):\n",
    "        if (T-i) % step_size == 0:\n",
    "            k = (T-i) // step_size\n",
    "            backg = background.copy()\n",
    "            heat_bg = [heat_bgs[i-1]]\n",
    "\n",
    "            img_sample = overlay_goal_agent(backg, wastes, goals.cpu(), x.cpu(), dot_size)\n",
    "            heat_sample = overlay_goal_agent(heat_bg, wastes, goals.cpu(), x.cpu(), dot_size)\n",
    "            for s in range(len(img_sample)):\n",
    "                if len(wastes) == 1:\n",
    "                    axs[0,k].imshow(img_sample[s])\n",
    "                    axs[0,k].set_title(f't = {T-i+1}')\n",
    "                    axs[0,k].axis('off')\n",
    "                    axs[1,k].imshow(heat_sample[s])\n",
    "                    axs[1,k].set_title(f't = {T-i+1}')\n",
    "                    axs[1,k].axis('off')\n",
    "                else:\n",
    "                    axs[s,k].imshow(img_sample[s])\n",
    "                    axs[0,k].set_title(f't = {T-i+1}')\n",
    "                    axs[s,k].axis('off')\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "\n",
    "        alpha = epsilon / diffusion.std[i-1] * (img_size/2)\n",
    "        \n",
    "        for _ in range(lan_t):\n",
    "            x_prev = x.clone()\n",
    "            score, _ = model(obs_T, t, x)\n",
    "            \n",
    "            x = x_prev + (alpha * score)/2 + (torch.randn_like(x) * torch.sqrt(alpha) * 0.25)\n",
    "\n",
    "            if not (x.abs() <=0.99).all():        \n",
    "                x = x_prev\n",
    "            bkg = background.copy()\n",
    "            ims.append(overlay_goal_agent(bkg, wastes, goals.cpu(), x.cpu(), dot_size))\n",
    "            ims_heat.append(overlay_goal_agent(heat_bg, wastes, goals.cpu(), x.cpu(), dot_size))\n",
    "            \n",
    "\n",
    "    backg = background.copy()\n",
    "    heat_bg = [heat_bgs[i-1]]\n",
    "    img_sample = overlay_goal_agent(backg, wastes, goals.cpu(),x.cpu(), dot_size)\n",
    "    heat_sample = overlay_goal_agent(heat_bg, wastes, goals.cpu(), x.cpu(), dot_size)\n",
    "    for s in range(len(img_sample)):\n",
    "        if len(wastes) == 1:\n",
    "            axs[0,-1].imshow(img_sample[s])\n",
    "            axs[0,-1].set_title(f't = {T}')\n",
    "            axs[0,-1].axis('off')    \n",
    "            axs[1,-1].imshow(heat_sample[s])\n",
    "            axs[1,-1].set_title(f't = {T}')\n",
    "            axs[1,-1].axis('off')\n",
    "        else:\n",
    "            axs[s,-1].imshow(img_sample[s])\n",
    "            axs[0,-1].set_title(f't = {T}')\n",
    "            axs[s,-1].axis('off')\n",
    "\n",
    "    bkg = background.copy()\n",
    "    ims.append(overlay_goal_agent(bkg, wastes, goals.cpu(), x.cpu(), dot_size))\n",
    "    ims_heat.append(overlay_goal_agent(heat_bg, wastes, goals.cpu(), x.cpu(), dot_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/heat/eval.npy', ims)\n",
    "np.save('./results/heat/heat.npy', ims_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the whole field\n",
    "\n",
    "i = 3\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y = np.linspace(-1, 1, img_size)\n",
    "x = np.linspace(-1, 1, img_size)\n",
    "xx, yy = np.meshgrid(x,y)\n",
    "coordinates = np.vstack((xx.ravel(), yy.ravel())).T\n",
    "coordinates = torch.tensor(coordinates).float().to(device)\n",
    "\n",
    "batch_size = 1024\n",
    "all_scores = []\n",
    "for j in tqdm(range(0, coordinates.size(0), batch_size)):\n",
    "    coords_batch = coordinates[j:j+batch_size]\n",
    "    \n",
    "    score_batch = []\n",
    "    for coord in coords_batch:\n",
    "        single_coord = coord.unsqueeze(0).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            score, _ = model(obs_T, (torch.ones(1) * i).long().to(device), single_coord)\n",
    "        score_batch.append(score.squeeze(1))\n",
    "        \n",
    "    all_scores.append(torch.stack(score_batch))\n",
    "scores = torch.cat(all_scores, dim=0)\n",
    "scores = scores.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the whole field\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "scores_x = scores[:, 1].reshape(img_size, img_size)\n",
    "scores_y = scores[:, 0].reshape(img_size, img_size)\n",
    "score_clip_x, score_clip_y = clip_vectors(scores_x, scores_y, 0.01)\n",
    "axes[0].quiver(xx, yy, score_clip_x.detach().cpu().numpy(), score_clip_y.detach().cpu().numpy(), angles='xy', scale_units='xy', scale=0.5)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlim(-1,1)\n",
    "axes[0].set_ylim(-1,1)\n",
    "\n",
    "# Ground truth\n",
    "obstacle_masks = convert_to_obstacle_masks(noise_steps, bg.size, img_size, obstacle_pos)\n",
    "background = draw_obstacles_pixel(bg, obstacle_masks)\n",
    "goal = torch.tensor([[[-0.7,-0.7]]]*noise_steps, device=device)\n",
    "obs = overlay_goal(background, img_size, wastes, goal)\n",
    "t = torch.tensor([i for i in range(1, noise_steps+1)], device=device)\n",
    "_, _, score_field, _ = diffusion.forward_diffusion(t, goal, sample_num, obstacle_masks)\n",
    "data = score_field[i]\n",
    "V = data[...,0]\n",
    "U = data[...,1]\n",
    "V_clip, U_clip = clip_vectors(V, U, 0.005)\n",
    "# V_clip, U_clip = V, U\n",
    "x, y = np.meshgrid(np.linspace(0, img_size-1, img_size), np.linspace(0,img_size-1,img_size))\n",
    "axes[1].quiver(x, y, U_clip.cpu().numpy(), V_clip.cpu().numpy(), angles='xy', scale_units='xy', scale=0.01)\n",
    "# axes[1].invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation - Multi goals\n",
    "\n",
    "c = 10\n",
    "fig,axs = plt.subplots(1, c + 1, figsize=(20,20))\n",
    "axs = axs.flatten()\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 2\n",
    "\n",
    "goals = gen_goals(goal_bounds, goal_num)\n",
    "\n",
    "objs = wastes.copy()\n",
    "gs = goals.clone()\n",
    "\n",
    "obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # x_T = torch.tensor([[0.5, 0.5]], device=device, dtype=torch.float32)\n",
    "    x_T = gen_goals(agent_bounds, n=(1, eval_samples)).unsqueeze(0)\n",
    "    x = x_T\n",
    "    \n",
    "    imgs=[]\n",
    "    for i in tqdm(reversed(range(1, noise_steps)), position=0):\n",
    "        if (T-1-i) % step_size == 0:\n",
    "            k = (T-1-i) // step_size\n",
    "            img_sample = overlay_goals_agent(bg, objs, gs.cpu(), x.cpu(), dot_size)\n",
    "            axs[k].imshow(img_sample)\n",
    "            axs[k].set_title(f't = {T-1-i}')\n",
    "            axs[k].axis('off')\n",
    "            imgs.append(img_sample)\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "        while True:\n",
    "            predicted_noise = model(obs_T, x, t)\n",
    "            alpha = diffusion.alpha[t]\n",
    "            alpha_hat = diffusion.alpha_hat[t]\n",
    "            beta = diffusion.beta[t]\n",
    "            if i > 1:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) \\\n",
    "                        * predicted_noise) + torch.sqrt(beta) * noise\n",
    "            # if (abs(x[0][0]) <= 1.) & (abs(x[0][1]) <= 1.):\n",
    "            #     break\n",
    "            mask = (x.abs() > 1)\n",
    "            x[mask] = torch.clamp(x[mask], min=-.9, max=.9)\n",
    "            if (x.abs() <=1).all():\n",
    "                break\n",
    "\n",
    "#         exclude_idx = -1\n",
    "#         for i in range(len(gs)):\n",
    "#             if get_distance(x[0], gs[i]) < 0.1 and len(gs) > 1:\n",
    "#                 exclude_idx = i\n",
    "#                 break\n",
    "#         if exclude_idx > -1:\n",
    "#             objs = objs[:i] + objs[i+1:]\n",
    "#             gs = torch.cat([gs[:i], gs[i+1:]], dim=0)\n",
    "#             obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "    img_sample = overlay_goals_agent(bg, objs, gs.cpu(),x.cpu(), dot_size)\n",
    "    axs[-1].imshow(img_sample)\n",
    "    axs[-1].set_title(f't = {T}')\n",
    "    axs[-1].axis('off')\n",
    "    imgs.append(img_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
