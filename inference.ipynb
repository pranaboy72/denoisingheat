{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt:tensor([    8,    50,   162,   392,   882,  1568,  2888,  5202,  8712, 14450],\n",
      "       device='cuda:0')\n",
      "heat kernel std:tensor([ 2.,  5.,  9., 14., 14., 14., 14., 14., 14., 14.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from scorefield.models.ddpm.denoising_diffusion import Unet\n",
    "from scorefield.models.heat.heat_diffusion import HeatDiffusion, HeatDiffusion_Revised\n",
    "from scorefield.utils.rl_utils import load_config\n",
    "from scorefield.utils.utils import (\n",
    "    gen_goals, overlay_goal, overlay_multiple, combine_objects, overlay_images,\n",
    "    overlay_goal_agent, overlay_goals_agent, log_num_check,\n",
    "    draw_obstacles_pil, convert_to_obstacle_masks,\n",
    "    randgen_obstacle_masks, draw_obstacles_pixel,\n",
    "    vector_field, clip_vectors\n",
    ")\n",
    "from scorefield.utils.diffusion_utils import bilinear_interpolate, bilinear_interpolate_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from typing import Optional\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Args\n",
    "config_dir = \"./scorefield/configs/heat_diffusion.yaml\"\n",
    "args = load_config(config_dir)\n",
    "device = args['device']\n",
    "\n",
    "bg = Image.open('assets/toy_exp/background0.png')\n",
    "wastes = []\n",
    "wastes.append(Image.open('assets/toy_exp/waste0.png'))\n",
    "# wastes.append(Image.open('assets/toy_exp/waste4.png'))\n",
    "# wastes.append(Image.open('assets/toy_exp/waste5.png'))\n",
    "\n",
    "\n",
    "img_size = args['image_size']\n",
    "goal_bounds = args['goal_bounds']\n",
    "goal_num = len(wastes)\n",
    "agent_bounds = args['agent_bounds']\n",
    "obstacle_pos = args['obstacles']\n",
    "\n",
    "model_path = os.path.join(args['log_path'], args['model_path'])\n",
    "\n",
    "u0 = args['u0']\n",
    "min_heat_step = args['min_heat_step']\n",
    "max_heat_step = args['max_heat_step']\n",
    "noise_steps = args['noise_steps']\n",
    "sample_num = args['sample_num']\n",
    "time_type = args['time_type']\n",
    "\n",
    "train_lr = args['train_lr']\n",
    "batch_size = noise_steps #args['batch_size']\n",
    "\n",
    "class Unet2D(Unet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        out_dim, \n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "    ):\n",
    "        super().__init__(dim=dim, out_dim=out_dim, dim_mults=dim_mults)\n",
    "\n",
    "    def forward(self, obs, t, x_t:Optional[torch.Tensor]=None):\n",
    "        score_map = super().forward(obs, t)\n",
    "        if x_t is not None:\n",
    "            score = bilinear_interpolate_samples(score_map, x_t)    # output: (B,2)\n",
    "            return score, score_map.permute(0,2,3,1)\n",
    "        else:\n",
    "            return score_map.permute(0, 2, 3, 1)\n",
    "    \n",
    "model = Unet2D(\n",
    "    dim=img_size,\n",
    "    out_dim = 2,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    ").to(device)\n",
    "\n",
    "diffusion = HeatDiffusion_Revised(\n",
    "    image_size=img_size,\n",
    "    u0 = u0,\n",
    "    noise_steps=noise_steps,\n",
    "    min_heat_step=min_heat_step,\n",
    "    max_heat_step=max_heat_step,\n",
    "    time_type=time_type,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./runs/Sep10_08-44-34_workspace-ryae9dru7a3u-0/model_params.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc4ElEQVR4nO3df2yV9d3/8Vdr20MFekornLajZTWiBRGGBcoZuDnobLiNgVEdGsyYIxJZQYEtahMFtzjLNAri+KHOgWYyJksAMTcwUqXGrSBUiSizgjZrZzkHXew5pbOHSj/fP7w9X4/CtlMOvjnH5yO5Enpd17n6/oTkPHP1nNOmOeecAAD4iqVbDwAA+HoiQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMZ5+rCq1ev1kMPPaRAIKAxY8boscce04QJE/7j43p7e9Xe3q6BAwcqLS3tXI0HADhHnHPq7OxUUVGR0tP/zX2OOwc2bdrksrKy3O9+9zv31ltvuVtvvdXl5ua6YDD4Hx/b1tbmJLGxsbGxJfnW1tb2b5/v05xL/C8jraio0Pjx4/Wb3/xG0qd3NcXFxVq4cKHuvvvuf/vYUCik3NxcTdb/KEOZiR4NAHCOfaIevaL/VUdHh7xe7xnPS/iP4E6ePKmmpibV1tZG96Wnp6uyslKNjY1fOj8SiSgSiUS/7uzs/L/BMpWRRoAAIOn8323Nf3oZJeFvQvjwww916tQp+Xy+mP0+n0+BQOBL59fV1cnr9Ua34uLiRI8EADgPmb8Lrra2VqFQKLq1tbVZjwQA+Aok/EdwF110kS644AIFg8GY/cFgUAUFBV863+PxyOPxJHoMAMB5LuF3QFlZWSovL1d9fX10X29vr+rr6+X3+xP97QAASeqcfA5oyZIlmjNnjsaNG6cJEyZo5cqV6urq0i233HIuvh0AIAmdkwDNmjVLH3zwgZYuXapAIKBvfetb2rlz55femAAA+Po6J58DOhvhcFher1dXazpvwwaAJPSJ69EebVMoFFJOTs4ZzzN/FxwA4OuJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJiIO0Avv/yyrrvuOhUVFSktLU1bt26NOe6c09KlS1VYWKjs7GxVVlbqyJEjiZoXAJAi4g5QV1eXxowZo9WrV5/2+IMPPqhVq1Zp3bp12rdvn/r376+qqip1d3ef9bAAgNSREe8Dpk2bpmnTpp32mHNOK1eu1D333KPp06dLkp555hn5fD5t3bpVN95445ceE4lEFIlEol+Hw+F4RwIAJKGEvgbU0tKiQCCgysrK6D6v16uKigo1Njae9jF1dXXyer3Rrbi4OJEjAQDOUwkNUCAQkCT5fL6Y/T6fL3rsi2praxUKhaJbW1tbIkcCAJyn4v4RXKJ5PB55PB7rMQAAX7GE3gEVFBRIkoLBYMz+YDAYPQYAgJTgAJWWlqqgoED19fXRfeFwWPv27ZPf70/ktwIAJLm4fwR34sQJHT16NPp1S0uLDh48qLy8PJWUlGjRokW6//77NXz4cJWWluree+9VUVGRZsyYkci5AQBJLu4AHThwQN/73veiXy9ZskSSNGfOHG3YsEF33nmnurq6NG/ePHV0dGjy5MnauXOn+vXrl7ipAQBJL80556yH+LxwOCyv16urNV0ZaZnW4wAA4vSJ69EebVMoFFJOTs4Zz+N3wQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJiIK0B1dXUaP368Bg4cqCFDhmjGjBlqbm6OOae7u1s1NTXKz8/XgAEDVF1drWAwmNChAQDJL64ANTQ0qKamRnv37tXu3bvV09Oja665Rl1dXdFzFi9erO3bt2vz5s1qaGhQe3u7Zs6cmfDBAQDJLc055/r64A8++EBDhgxRQ0ODvvOd7ygUCmnw4MHauHGjrr/+eknS22+/rREjRqixsVETJ078j9cMh8Pyer26WtOVkZbZ19EAAEY+cT3ao20KhULKyck543ln9RpQKBSSJOXl5UmSmpqa1NPTo8rKyug5ZWVlKikpUWNj42mvEYlEFA6HYzYAQOrrc4B6e3u1aNEiTZo0SaNGjZIkBQIBZWVlKTc3N+Zcn8+nQCBw2uvU1dXJ6/VGt+Li4r6OBABIIn0OUE1Njd58801t2rTprAaora1VKBSKbm1tbWd1PQBAcsjoy4MWLFigF154QS+//LKGDh0a3V9QUKCTJ0+qo6Mj5i4oGAyqoKDgtNfyeDzyeDx9GQMAkMTiugNyzmnBggXasmWLXnzxRZWWlsYcLy8vV2Zmpurr66P7mpub1draKr/fn5iJAQApIa47oJqaGm3cuFHbtm3TwIEDo6/reL1eZWdny+v1au7cuVqyZIny8vKUk5OjhQsXyu/3/1fvgAMAfH3EFaC1a9dKkq6++uqY/evXr9ePf/xjSdKKFSuUnp6u6upqRSIRVVVVac2aNQkZFgCQOs7qc0DnAp8DAoDk9pV8DggAgL4iQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACbiCtDatWs1evRo5eTkKCcnR36/Xzt27Ige7+7uVk1NjfLz8zVgwABVV1crGAwmfGgAQPKLK0BDhw7V8uXL1dTUpAMHDmjKlCmaPn263nrrLUnS4sWLtX37dm3evFkNDQ1qb2/XzJkzz8ngAIDkluacc2dzgby8PD300EO6/vrrNXjwYG3cuFHXX3+9JOntt9/WiBEj1NjYqIkTJ/5X1wuHw/J6vbpa05WRlnk2owEADHzierRH2xQKhZSTk3PG8/r8GtCpU6e0adMmdXV1ye/3q6mpST09PaqsrIyeU1ZWppKSEjU2Np7xOpFIROFwOGYDAKS+uAN06NAhDRgwQB6PR7fddpu2bNmikSNHKhAIKCsrS7m5uTHn+3w+BQKBM16vrq5OXq83uhUXF8e9CABA8ok7QJdddpkOHjyoffv2af78+ZozZ44OHz7c5wFqa2sVCoWiW1tbW5+vBQBIHhnxPiArK0uXXHKJJKm8vFz79+/Xo48+qlmzZunkyZPq6OiIuQsKBoMqKCg44/U8Ho88Hk/8kwMAktpZfw6ot7dXkUhE5eXlyszMVH19ffRYc3OzWltb5ff7z/bbAABSTFx3QLW1tZo2bZpKSkrU2dmpjRs3as+ePdq1a5e8Xq/mzp2rJUuWKC8vTzk5OVq4cKH8fv9//Q44AMDXR1wBOn78uH70ox/p2LFj8nq9Gj16tHbt2qXvf//7kqQVK1YoPT1d1dXVikQiqqqq0po1a87J4ACA5HbWnwNKND4HBADJ7Zx/DggAgLNBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEycVYCWL1+utLQ0LVq0KLqvu7tbNTU1ys/P14ABA1RdXa1gMHi2cwIAUkyfA7R//349/vjjGj16dMz+xYsXa/v27dq8ebMaGhrU3t6umTNnnvWgAIDU0qcAnThxQrNnz9aTTz6pQYMGRfeHQiE99dRTeuSRRzRlyhSVl5dr/fr1+utf/6q9e/cmbGgAQPLrU4Bqamp07bXXqrKyMmZ/U1OTenp6YvaXlZWppKREjY2Np71WJBJROByO2QAAqS8j3gds2rRJr732mvbv3/+lY4FAQFlZWcrNzY3Z7/P5FAgETnu9uro6/eIXv4h3DABAkovrDqitrU133HGHnn32WfXr1y8hA9TW1ioUCkW3tra2hFwXAHB+iytATU1NOn78uK688kplZGQoIyNDDQ0NWrVqlTIyMuTz+XTy5El1dHTEPC4YDKqgoOC01/R4PMrJyYnZAACpL64fwU2dOlWHDh2K2XfLLbeorKxMd911l4qLi5WZman6+npVV1dLkpqbm9Xa2iq/35+4qQEASS+uAA0cOFCjRo2K2de/f3/l5+dH98+dO1dLlixRXl6ecnJytHDhQvn9fk2cODFxUwMAkl7cb0L4T1asWKH09HRVV1crEomoqqpKa9asSfS3AQAkuTTnnLMe4vPC4bC8Xq+u1nRlpGVajwMAiNMnrkd7tE2hUOjfvq7P74IDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACbiCtB9992ntLS0mK2srCx6vLu7WzU1NcrPz9eAAQNUXV2tYDCY8KEBAMkv7jugyy+/XMeOHYtur7zySvTY4sWLtX37dm3evFkNDQ1qb2/XzJkzEzowACA1ZMT9gIwMFRQUfGl/KBTSU089pY0bN2rKlCmSpPXr12vEiBHau3evJk6ceNrrRSIRRSKR6NfhcDjekQAASSjuO6AjR46oqKhIF198sWbPnq3W1lZJUlNTk3p6elRZWRk9t6ysTCUlJWpsbDzj9erq6uT1eqNbcXFxH5YBAEg2cQWooqJCGzZs0M6dO7V27Vq1tLToqquuUmdnpwKBgLKyspSbmxvzGJ/Pp0AgcMZr1tbWKhQKRbe2trY+LQQAkFzi+hHctGnTov8ePXq0KioqNGzYMD333HPKzs7u0wAej0cej6dPjwUAJK+zeht2bm6uLr30Uh09elQFBQU6efKkOjo6Ys4JBoOnfc0IAPD1dlYBOnHihN59910VFhaqvLxcmZmZqq+vjx5vbm5Wa2ur/H7/WQ8KAEgtcf0I7uc//7muu+46DRs2TO3t7Vq2bJkuuOAC3XTTTfJ6vZo7d66WLFmivLw85eTkaOHChfL7/Wd8BxwA4OsrrgD94x//0E033aR//vOfGjx4sCZPnqy9e/dq8ODBkqQVK1YoPT1d1dXVikQiqqqq0po1a87J4ACA5JbmnHPWQ3xeOByW1+vV1ZqujLRM63EAAHH6xPVoj7YpFAopJyfnjOfxu+AAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMxB2g999/XzfffLPy8/OVnZ2tK664QgcOHIged85p6dKlKiwsVHZ2tiorK3XkyJGEDg0ASH5xBeijjz7SpEmTlJmZqR07dujw4cN6+OGHNWjQoOg5Dz74oFatWqV169Zp37596t+/v6qqqtTd3Z3w4QEAySsjnpN//etfq7i4WOvXr4/uKy0tjf7bOaeVK1fqnnvu0fTp0yVJzzzzjHw+n7Zu3aobb7wxQWMDAJJdXHdAzz//vMaNG6cbbrhBQ4YM0dixY/Xkk09Gj7e0tCgQCKiysjK6z+v1qqKiQo2Njae9ZiQSUTgcjtkAAKkvrgC99957Wrt2rYYPH65du3Zp/vz5uv322/X0009LkgKBgCTJ5/PFPM7n80WPfVFdXZ28Xm90Ky4u7ss6AABJJq4A9fb26sorr9QDDzygsWPHat68ebr11lu1bt26Pg9QW1urUCgU3dra2vp8LQBA8ogrQIWFhRo5cmTMvhEjRqi1tVWSVFBQIEkKBoMx5wSDweixL/J4PMrJyYnZAACpL64ATZo0Sc3NzTH73nnnHQ0bNkzSp29IKCgoUH19ffR4OBzWvn375Pf7EzAuACBVxPUuuMWLF+vb3/62HnjgAf3whz/Uq6++qieeeEJPPPGEJCktLU2LFi3S/fffr+HDh6u0tFT33nuvioqKNGPGjHMxPwAgScUVoPHjx2vLli2qra3VL3/5S5WWlmrlypWaPXt29Jw777xTXV1dmjdvnjo6OjR58mTt3LlT/fr1S/jwAIDkleacc9ZDfF44HJbX69XVmq6MtEzrcQAAcfrE9WiPtikUCv3b1/X5XXAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIm4fhv2V+Gz3436iXqk8+rXpAIA/hufqEfS/38+P5PzLkCdnZ2SpFf0v8aTAADORmdnp7xe7xmPn3d/jqG3t1ft7e0aOHCgOjs7VVxcrLa2tpT+U93hcJh1poivwxol1plqEr1O55w6OztVVFSk9PQzv9Jz3t0Bpaena+jQoZI+/QurkpSTk5PS//mfYZ2p4+uwRol1pppErvPf3fl8hjchAABMECAAgInzOkAej0fLli2Tx+OxHuWcYp2p4+uwRol1phqrdZ53b0IAAHw9nNd3QACA1EWAAAAmCBAAwAQBAgCYIEAAABPndYBWr16tb37zm+rXr58qKir06quvWo90Vl5++WVdd911KioqUlpamrZu3Rpz3DmnpUuXqrCwUNnZ2aqsrNSRI0dshu2juro6jR8/XgMHDtSQIUM0Y8YMNTc3x5zT3d2tmpoa5efna8CAAaqurlYwGDSauG/Wrl2r0aNHRz857vf7tWPHjujxVFjjFy1fvlxpaWlatGhRdF8qrPO+++5TWlpazFZWVhY9ngpr/Mz777+vm2++Wfn5+crOztYVV1yhAwcORI9/1c9B522A/vjHP2rJkiVatmyZXnvtNY0ZM0ZVVVU6fvy49Wh91tXVpTFjxmj16tWnPf7ggw9q1apVWrdunfbt26f+/furqqpK3d3dX/GkfdfQ0KCamhrt3btXu3fvVk9Pj6655hp1dXVFz1m8eLG2b9+uzZs3q6GhQe3t7Zo5c6bh1PEbOnSoli9frqamJh04cEBTpkzR9OnT9dZbb0lKjTV+3v79+/X4449r9OjRMftTZZ2XX365jh07Ft1eeeWV6LFUWeNHH32kSZMmKTMzUzt27NDhw4f18MMPa9CgQdFzvvLnIHeemjBhgqupqYl+ferUKVdUVOTq6uoMp0ocSW7Lli3Rr3t7e11BQYF76KGHovs6Ojqcx+Nxf/jDHwwmTIzjx487Sa6hocE59+maMjMz3ebNm6Pn/O1vf3OSXGNjo9WYCTFo0CD329/+NuXW2NnZ6YYPH+52797tvvvd77o77rjDOZc6/5fLli1zY8aMOe2xVFmjc87dddddbvLkyWc8bvEcdF7eAZ08eVJNTU2qrKyM7ktPT1dlZaUaGxsNJzt3WlpaFAgEYtbs9XpVUVGR1GsOhUKSpLy8PElSU1OTenp6YtZZVlamkpKSpF3nqVOntGnTJnV1dcnv96fcGmtqanTttdfGrEdKrf/LI0eOqKioSBdffLFmz56t1tZWSam1xueff17jxo3TDTfcoCFDhmjs2LF68skno8ctnoPOywB9+OGHOnXqlHw+X8x+n8+nQCBgNNW59dm6UmnNvb29WrRokSZNmqRRo0ZJ+nSdWVlZys3NjTk3Gdd56NAhDRgwQB6PR7fddpu2bNmikSNHptQaN23apNdee011dXVfOpYq66yoqNCGDRu0c+dOrV27Vi0tLbrqqqvU2dmZMmuUpPfee09r167V8OHDtWvXLs2fP1+33367nn76aUk2z0Hn3Z9jQOqoqanRm2++GfPz9FRy2WWX6eDBgwqFQvrTn/6kOXPmqKGhwXqshGlra9Mdd9yh3bt3q1+/ftbjnDPTpk2L/nv06NGqqKjQsGHD9Nxzzyk7O9twssTq7e3VuHHj9MADD0iSxo4dqzfffFPr1q3TnDlzTGY6L++ALrroIl1wwQVfeqdJMBhUQUGB0VTn1mfrSpU1L1iwQC+88IJeeuml6N93kj5d58mTJ9XR0RFzfjKuMysrS5dcconKy8tVV1enMWPG6NFHH02ZNTY1Nen48eO68sorlZGRoYyMDDU0NGjVqlXKyMiQz+dLiXV+UW5uri699FIdPXo0Zf4vJamwsFAjR46M2TdixIjojxstnoPOywBlZWWpvLxc9fX10X29vb2qr6+X3+83nOzcKS0tVUFBQcyaw+Gw9u3bl1Rrds5pwYIF2rJli1588UWVlpbGHC8vL1dmZmbMOpubm9Xa2ppU6zyd3t5eRSKRlFnj1KlTdejQIR08eDC6jRs3TrNnz47+OxXW+UUnTpzQu+++q8LCwpT5v5SkSZMmfekjEe+8846GDRsmyeg56Jy8tSEBNm3a5Dwej9uwYYM7fPiwmzdvnsvNzXWBQMB6tD7r7Ox0r7/+unv99dedJPfII4+4119/3f397393zjm3fPlyl5ub67Zt2+beeOMNN336dFdaWuo+/vhj48n/e/Pnz3der9ft2bPHHTt2LLr961//ip5z2223uZKSEvfiiy+6AwcOOL/f7/x+v+HU8bv77rtdQ0ODa2lpcW+88Ya7++67XVpamvvzn//snEuNNZ7O598F51xqrPNnP/uZ27Nnj2tpaXF/+ctfXGVlpbvooovc8ePHnXOpsUbnnHv11VddRkaG+9WvfuWOHDninn32WXfhhRe63//+99FzvurnoPM2QM4599hjj7mSkhKXlZXlJkyY4Pbu3Ws90ll56aWXnKQvbXPmzHHOffo2yHvvvdf5fD7n8Xjc1KlTXXNzs+3QcTrd+iS59evXR8/5+OOP3U9/+lM3aNAgd+GFF7of/OAH7tixY3ZD98FPfvITN2zYMJeVleUGDx7spk6dGo2Pc6mxxtP5YoBSYZ2zZs1yhYWFLisry33jG99ws2bNckePHo0eT4U1fmb79u1u1KhRzuPxuLKyMvfEE0/EHP+qn4P4e0AAABPn5WtAAIDUR4AAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMT/A+/R+Kt6EN5LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation pre-setting\n",
    "\n",
    "# background = draw_obstacles_pil(bg, obstacle_pos)\n",
    "# obstacle_masks = convert_to_obstacle_masks(noise_steps, background[0].size, img_size, obstacle_pos)\n",
    "obstacle_masks = randgen_obstacle_masks(1, img_size)\n",
    "background = draw_obstacles_pixel(bg, obstacle_masks)\n",
    "\n",
    "obstacle_masks = obstacle_masks[0].unsqueeze(0)\n",
    "\n",
    "plt.imshow(obstacle_masks[0].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:17,  2.51s/it]"
     ]
    }
   ],
   "source": [
    "# Evaluation - Single goal\n",
    "\n",
    "lan_t = 200  # 4\n",
    "epsilon = .01\n",
    "\n",
    "goals = torch.tensor([[[-0.8, -0.8], [-0.8, 0.8]]], device=device, dtype=torch.float32)\n",
    "obs_T = overlay_goal(background, img_size, wastes, goals)\n",
    "\n",
    "c = 10\n",
    "fig, axs = plt.subplots(goal_num, c, figsize=(20,20))\n",
    "\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 3\n",
    "\n",
    "\n",
    "model.eval()\n",
    "ims = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_T = torch.tensor([[[0., 0.]]], device=device, dtype=torch.float32)\n",
    "    x = x_T\n",
    "    \n",
    "    for i in tqdm(reversed(range(1, T+1)), position=0):\n",
    "        if (T-i) % step_size == 0:\n",
    "            k = (T-i) // step_size\n",
    "            backg = background.copy()\n",
    "\n",
    "            img_sample = overlay_goal_agent(backg, wastes, goals.cpu(), x.cpu(), dot_size)\n",
    "            for s in range(len(img_sample)):\n",
    "                if len(wastes) == 1:\n",
    "                    axs[k].imshow(img_sample[s])\n",
    "                    axs[k].set_title(f't = {T-i+1}')\n",
    "                    axs[k].axis('off')    \n",
    "                else:\n",
    "                    axs[s,k].imshow(img_sample[s])\n",
    "                    axs[0,k].set_title(f't = {T-i+1}')\n",
    "                    axs[s,k].axis('off')\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "\n",
    "        alpha = epsilon / diffusion.std[i-1] * (img_size/2)\n",
    "        \n",
    "        for _ in range(lan_t):\n",
    "            x_prev = x.clone()\n",
    "            score, _ = model(obs_T, t, x)\n",
    "            \n",
    "            x = x_prev + (alpha * score)/2 + (torch.randn_like(x) * torch.sqrt(alpha) * 0.2)\n",
    "\n",
    "            if not (x.abs() <=0.99).all():        \n",
    "                x = x_prev\n",
    "            bkg = background.copy()\n",
    "            im = overlay_goal_agent(bkg, wastes, goals.cpu(), x.cpu(), dot_size)\n",
    "            ims.append(im)\n",
    "\n",
    "    backg = background.copy()\n",
    "    img_sample = overlay_goal_agent(backg, wastes, goals.cpu(),x.cpu(), dot_size)\n",
    "    for s in range(len(img_sample)):\n",
    "        if len(img_sample) == 1:\n",
    "            axs[-1].imshow(img_sample[s])\n",
    "            axs[-1].set_title(f't = {T}')\n",
    "            axs[-1].axis('off')    \n",
    "        else:\n",
    "            axs[s,-1].imshow(img_sample[s])\n",
    "            axs[0,-1].set_title(f't = {T}')\n",
    "            axs[s,-1].axis('off')\n",
    "\n",
    "    bkg = background.copy()\n",
    "    im = overlay_goal_agent(bkg, wastes, goals.cpu(), x.cpu(), dot_size)       \n",
    "    ims.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:518: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.asanyarray(arr)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "np.save('./results/heat/eval.npy', ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the whole field\n",
    "\n",
    "i = 3\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y = np.linspace(-1, 1, img_size)\n",
    "x = np.linspace(-1, 1, img_size)\n",
    "xx, yy = np.meshgrid(x,y)\n",
    "coordinates = np.vstack((xx.ravel(), yy.ravel())).T\n",
    "coordinates = torch.tensor(coordinates).float().to(device)\n",
    "\n",
    "batch_size = 1024\n",
    "all_scores = []\n",
    "for j in tqdm(range(0, coordinates.size(0), batch_size)):\n",
    "    coords_batch = coordinates[j:j+batch_size]\n",
    "    \n",
    "    score_batch = []\n",
    "    for coord in coords_batch:\n",
    "        single_coord = coord.unsqueeze(0).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            score, _ = model(obs_T, (torch.ones(1) * i).long().to(device), single_coord)\n",
    "        score_batch.append(score.squeeze(1))\n",
    "        \n",
    "    all_scores.append(torch.stack(score_batch))\n",
    "scores = torch.cat(all_scores, dim=0)\n",
    "scores = scores.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the whole field\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "scores_x = scores[:, 1].reshape(img_size, img_size)\n",
    "scores_y = scores[:, 0].reshape(img_size, img_size)\n",
    "score_clip_x, score_clip_y = clip_vectors(scores_x, scores_y, 0.01)\n",
    "axes[0].quiver(xx, yy, score_clip_x.detach().cpu().numpy(), score_clip_y.detach().cpu().numpy(), angles='xy', scale_units='xy', scale=0.5)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlim(-1,1)\n",
    "axes[0].set_ylim(-1,1)\n",
    "\n",
    "# Ground truth\n",
    "obstacle_masks = convert_to_obstacle_masks(noise_steps, bg.size, img_size, obstacle_pos)\n",
    "background = draw_obstacles_pixel(bg, obstacle_masks)\n",
    "goal = torch.tensor([[[-0.7,-0.7]]]*noise_steps, device=device)\n",
    "obs = overlay_goal(background, img_size, wastes, goal)\n",
    "t = torch.tensor([i for i in range(1, noise_steps+1)], device=device)\n",
    "_, _, score_field, _ = diffusion.forward_diffusion(t, goal, sample_num, obstacle_masks)\n",
    "data = score_field[i]\n",
    "V = data[...,0]\n",
    "U = data[...,1]\n",
    "V_clip, U_clip = clip_vectors(V, U, 0.005)\n",
    "# V_clip, U_clip = V, U\n",
    "x, y = np.meshgrid(np.linspace(0, img_size-1, img_size), np.linspace(0,img_size-1,img_size))\n",
    "axes[1].quiver(x, y, U_clip.cpu().numpy(), V_clip.cpu().numpy(), angles='xy', scale_units='xy', scale=0.01)\n",
    "# axes[1].invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation - Multi goals\n",
    "\n",
    "c = 10\n",
    "fig,axs = plt.subplots(1, c + 1, figsize=(20,20))\n",
    "axs = axs.flatten()\n",
    "T = diffusion.noise_steps\n",
    "step_size = int(T / c)\n",
    "dot_size = 2\n",
    "\n",
    "goals = gen_goals(goal_bounds, goal_num)\n",
    "\n",
    "objs = wastes.copy()\n",
    "gs = goals.clone()\n",
    "\n",
    "obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # x_T = torch.tensor([[0.5, 0.5]], device=device, dtype=torch.float32)\n",
    "    x_T = gen_goals(agent_bounds, n=(1, eval_samples)).unsqueeze(0)\n",
    "    x = x_T\n",
    "    \n",
    "    imgs=[]\n",
    "    for i in tqdm(reversed(range(1, noise_steps)), position=0):\n",
    "        if (T-1-i) % step_size == 0:\n",
    "            k = (T-1-i) // step_size\n",
    "            img_sample = overlay_goals_agent(bg, objs, gs.cpu(), x.cpu(), dot_size)\n",
    "            axs[k].imshow(img_sample)\n",
    "            axs[k].set_title(f't = {T-1-i}')\n",
    "            axs[k].axis('off')\n",
    "            imgs.append(img_sample)\n",
    "\n",
    "        t = (torch.ones(1) * i).long().to(device)\n",
    "        while True:\n",
    "            predicted_noise = model(obs_T, x, t)\n",
    "            alpha = diffusion.alpha[t]\n",
    "            alpha_hat = diffusion.alpha_hat[t]\n",
    "            beta = diffusion.beta[t]\n",
    "            if i > 1:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) \\\n",
    "                        * predicted_noise) + torch.sqrt(beta) * noise\n",
    "            # if (abs(x[0][0]) <= 1.) & (abs(x[0][1]) <= 1.):\n",
    "            #     break\n",
    "            mask = (x.abs() > 1)\n",
    "            x[mask] = torch.clamp(x[mask], min=-.9, max=.9)\n",
    "            if (x.abs() <=1).all():\n",
    "                break\n",
    "\n",
    "#         exclude_idx = -1\n",
    "#         for i in range(len(gs)):\n",
    "#             if get_distance(x[0], gs[i]) < 0.1 and len(gs) > 1:\n",
    "#                 exclude_idx = i\n",
    "#                 break\n",
    "#         if exclude_idx > -1:\n",
    "#             objs = objs[:i] + objs[i+1:]\n",
    "#             gs = torch.cat([gs[:i], gs[i+1:]], dim=0)\n",
    "#             obs_T = overlay_images(bg, img_size, objs, gs)\n",
    "\n",
    "    img_sample = overlay_goals_agent(bg, objs, gs.cpu(),x.cpu(), dot_size)\n",
    "    axs[-1].imshow(img_sample)\n",
    "    axs[-1].set_title(f't = {T}')\n",
    "    axs[-1].axis('off')\n",
    "    imgs.append(img_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
